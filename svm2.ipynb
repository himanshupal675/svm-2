{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b4e8bec-f39f-426b-9e0d-f32de42b73f3",
   "metadata": {},
   "source": [
    "## Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cd3997-ce32-4c5a-8652-5c3f924a2e57",
   "metadata": {},
   "source": [
    "Polynomial functions and kernel functions are both mathematical tools used in machine learning, particularly in the context of support vector machines (SVMs) and kernel methods. While they serve different purposes, there is a connection between them through the use of kernel trick.\n",
    "\n",
    "1. Polynomial Functions:\n",
    "   - Polynomial functions are mathematical functions of the form f(x) = a_nx^n + a_(n-1)x^(n-1) + ... + a_1x + a_0, where \"x\" is the input variable, and \"n\" is a non-negative integer.\n",
    "   - Polynomial functions are used to capture complex relationships between input data and output, especially when the data doesn't fit well with linear models.\n",
    "\n",
    "2. Kernel Functions:\n",
    "   - Kernel functions are used in machine learning algorithms, particularly in SVMs, to transform input data into a higher-dimensional space without explicitly computing the transformation.\n",
    "   - They enable SVMs to find complex decision boundaries by mapping the data to a higher-dimensional space where the data might be linearly separable.\n",
    "\n",
    "Now, the relationship between polynomial functions and kernel functions in machine learning is established through the concept of the \"polynomial kernel.\"\n",
    "\n",
    "3. Polynomial Kernel:\n",
    "   - The polynomial kernel is a type of kernel function used in SVMs to implicitly map data into a higher-dimensional space using a polynomial function.\n",
    "   - The polynomial kernel is defined as K(x, y) = (x · y + c)^d, where \"x\" and \"y\" are input data points, \"c\" is a constant, and \"d\" is the degree of the polynomial.\n",
    "   - The polynomial kernel allows SVMs to learn decision boundaries in the transformed space, making it capable of capturing nonlinear relationships.\n",
    "\n",
    "In summary, polynomial functions can be used directly to model relationships in data, while polynomial kernels (a type of kernel function) are used in machine learning, particularly SVMs, to transform data into a higher-dimensional space to capture complex patterns and nonlinear relationships. The polynomial kernel effectively combines the concept of polynomial functions with kernel methods, allowing SVMs to work well for data that may not be linearly separable in the original feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63358db0-0352-4008-9d5d-6a3a4b077423",
   "metadata": {},
   "source": [
    "## Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c79e77-f202-43ed-8e04-9fc495dd15d5",
   "metadata": {},
   "source": [
    "You can implement an SVM with a polynomial kernel in Python using the Scikit-learn library. Scikit-learn provides a user-friendly interface for working with SVMs and various types of kernels, including the polynomial kernel. Here's a step-by-step guide on how to do it:\n",
    "\n",
    "1. Import the necessary libraries:\n",
    "\n",
    "```python\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "```\n",
    "\n",
    "2. Load or prepare your dataset. For this example, let's assume you have a dataset in the form of feature vectors `X` and corresponding labels `y`.\n",
    "\n",
    "3. Split the dataset into training and testing sets:\n",
    "\n",
    "```python\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "\n",
    "4. Create an SVM classifier with a polynomial kernel:\n",
    "\n",
    "```python\n",
    "# Create an SVM classifier with a polynomial kernel\n",
    "svm_classifier = SVC(kernel='poly', degree=3)  # 'poly' specifies the polynomial kernel, and 'degree' sets the polynomial degree\n",
    "```\n",
    "\n",
    "In the code above:\n",
    "- `kernel='poly'` specifies that you want to use a polynomial kernel.\n",
    "- `degree=3` sets the degree of the polynomial kernel. You can adjust this value based on your problem and data.\n",
    "\n",
    "5. Train the SVM classifier on your training data:\n",
    "\n",
    "```python\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "6. Make predictions on the test data:\n",
    "\n",
    "```python\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "```\n",
    "\n",
    "7. Evaluate the model's performance, e.g., using metrics like accuracy, precision, recall, or F1-score:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "# You can also print a detailed classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "```\n",
    "\n",
    "This code outlines the basic steps to implement an SVM with a polynomial kernel using Scikit-learn in Python. Remember to adjust the `degree` parameter and other hyperparameters as needed to fine-tune the model for your specific problem and dataset. Additionally, you may consider performing hyperparameter tuning using techniques like grid search or randomized search to find the best combination of hyperparameters for your SVM model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ad33eb-1612-48ef-8899-df6544491cd5",
   "metadata": {},
   "source": [
    "## Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b915e1-bf34-416f-a0fe-941964fd8f06",
   "metadata": {},
   "source": [
    "In Support Vector Regression (SVR), the value of epsilon (ε) is a hyperparameter that determines the width of the epsilon-insensitive tube around the regression line (or hyperplane) within which no penalties are incurred for data points that fall inside this tube. The epsilon-insensitive tube is a region where errors are considered negligible and don't contribute to the loss function.\n",
    "\n",
    "The number of support vectors in SVR can be affected by the value of epsilon as follows:\n",
    "\n",
    "1. **Smaller Epsilon (Tight Tube)**:\n",
    "   - When you set a smaller value for epsilon, it means that the epsilon-insensitive tube is narrower. This implies that the SVR model is more sensitive to deviations from the regression line.\n",
    "   - With a tighter tube, the SVR model might need more support vectors to capture the data points that fall outside the tube. This can result in a larger number of support vectors.\n",
    "   - A smaller epsilon can lead to a more complex model that tries to fit the training data more closely, potentially leading to overfitting.\n",
    "\n",
    "2. **Larger Epsilon (Wider Tube)**:\n",
    "   - Conversely, when you set a larger value for epsilon, the epsilon-insensitive tube becomes wider. This allows for larger deviations from the regression line without incurring penalties.\n",
    "   - With a wider tube, the SVR model may require fewer support vectors because it tolerates more errors and allows some data points to fall within the tube without penalty.\n",
    "   - A larger epsilon can lead to a simpler model with fewer support vectors. However, it may result in less accurate predictions if the data has substantial noise or outliers.\n",
    "\n",
    "In summary, the choice of epsilon in SVR influences the trade-off between model complexity and accuracy. Smaller epsilon values result in more complex models with more support vectors, while larger epsilon values lead to simpler models with fewer support vectors. The appropriate value of epsilon depends on the specific characteristics of your dataset and the desired balance between model complexity and generalization. It often requires experimentation and cross-validation to determine the optimal value for epsilon in SVR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a7792e-44f2-40b7-9176-fe8b4985f1fc",
   "metadata": {},
   "source": [
    "## Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works and provide examples of when you might want to increase or decrease its value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff11e09f-9daa-485a-be50-a142c9725553",
   "metadata": {},
   "source": [
    "Support Vector Regression (SVR) is a machine learning algorithm that relies on several hyperparameters to control its performance and flexibility. The choice of kernel function, C parameter, epsilon parameter (ε), and gamma parameter (γ) can significantly affect SVR's performance. Here's an explanation of each parameter and how it influences SVR, along with examples of when you might want to adjust its value:\n",
    "\n",
    "1. **Kernel Function**:\n",
    "   - The kernel function determines how the input space is transformed into a higher-dimensional space, allowing SVR to capture complex relationships.\n",
    "   - Common kernel functions include 'linear,' 'poly' (polynomial), 'rbf' (radial basis function), and 'sigmoid.'\n",
    "\n",
    "   Example:\n",
    "   - Use a 'linear' kernel when you believe the relationship between input and output is approximately linear.\n",
    "   - Use a 'poly' kernel with a higher degree when you suspect a polynomial relationship.\n",
    "   - Use an 'rbf' kernel when the relationship is nonlinear and you want to capture complex patterns.\n",
    "\n",
    "2. **C Parameter**:\n",
    "   - The C parameter controls the trade-off between achieving a small training error and a large-margin hyperplane.\n",
    "   - A smaller C allows for a wider margin but may tolerate more training errors, potentially leading to a simpler model.\n",
    "   - A larger C enforces a smaller-margin hyperplane and aims to minimize training errors, potentially leading to a more complex model.\n",
    "\n",
    "   Example:\n",
    "   - Increase C when you suspect the training data has minimal noise, and you want the SVR model to fit the training data closely.\n",
    "   - Decrease C when you want a more tolerant model that prioritizes a larger margin and is less influenced by individual data points.\n",
    "\n",
    "3. **Epsilon Parameter (ε)**:\n",
    "   - The epsilon parameter defines the width of the epsilon-insensitive tube around the regression line. Data points within this tube do not contribute to the loss function.\n",
    "   - A smaller ε results in a narrower tube, making the model more sensitive to errors within the tube.\n",
    "   - A larger ε results in a wider tube, allowing more data points to be within the tube without penalty.\n",
    "\n",
    "   Example:\n",
    "   - Increase ε when your data has noise or outliers, and you want the SVR model to be less sensitive to individual data points.\n",
    "   - Decrease ε when you have confidence in the data's accuracy and want the model to closely fit the training data.\n",
    "\n",
    "4. **Gamma Parameter (γ)**:\n",
    "   - The gamma parameter controls the shape of the radial basis function (RBF) kernel.\n",
    "   - A smaller γ results in a wider and smoother RBF kernel, capturing broader patterns in the data.\n",
    "   - A larger γ leads to a narrower and more localized RBF kernel, making the model more sensitive to fine-grained patterns.\n",
    "\n",
    "   Example:\n",
    "   - Increase γ when you believe that the relevant patterns in the data are localized and you want the SVR model to be sensitive to these local patterns.\n",
    "   - Decrease γ when you believe the relevant patterns are spread out and you want the model to capture broader trends.\n",
    "\n",
    "It's important to note that the optimal values for these parameters depend on your specific dataset and problem. Hyperparameter tuning techniques such as grid search or randomized search can help you find the best combination of these parameters to achieve the desired model performance and generalization. Additionally, using cross-validation is crucial to ensure that your SVR model performs well on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d25a31-2284-458b-a8a7-ff3de286719c",
   "metadata": {},
   "source": [
    "## Answer 5 (Assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f31da39-b9d6-4a24-b846-15025d73cb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9824561403508771\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.98        43\n",
      "           1       0.97      1.00      0.99        71\n",
      "\n",
      "    accuracy                           0.98       114\n",
      "   macro avg       0.99      0.98      0.98       114\n",
      "weighted avg       0.98      0.98      0.98       114\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['best_svc_classifier.pkl']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "\n",
    "# Load the Breast Cancer Wisconsin dataset\n",
    "data = datasets.load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data - Scale features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of the SVC classifier\n",
    "svc_classifier = SVC()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svc_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Use the trained classifier to predict the labels of the testing data\n",
    "y_pred = svc_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the classifier (accuracy and classification report)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Tune hyperparameters using GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'gamma': ['scale', 'auto'] + [0.001, 0.01, 0.1, 1]\n",
    "}\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and estimator from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "best_svc_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "best_svc_classifier.fit(X, y)\n",
    "\n",
    "# Save the trained classifier to a file (e.g., using joblib)\n",
    "joblib.dump(best_svc_classifier, 'best_svc_classifier.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e862d9-f1e1-4539-bf1a-888d8df97686",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
